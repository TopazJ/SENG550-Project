{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19ae405",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SENG 550 F2021 group 7 final project\n",
    "\n",
    "### Members\n",
    "- Jean-David Rousseau, 10157955\n",
    "- Victor Sanchez Alarcon, 30054425\n",
    "- Shamez Meghji, 30055991\n",
    "- Owen Troke-Billard, 30046207\n",
    "\n",
    "### Notebook contents\n",
    "1. Dependencies\n",
    "2. Import data\n",
    "3. Clean data\n",
    "4. Train model\n",
    "5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa513a",
   "metadata": {},
   "source": [
    "## 1. Dependencies\n",
    "\n",
    "Install required PIP packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd43ae7a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.6.5 in /opt/conda/lib/python3.9/site-packages (3.6.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk==3.6.5) (4.62.3)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk==3.6.5) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk==3.6.5) (1.1.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk==3.6.5) (2021.11.10)\r\n",
      "Requirement already satisfied: sklearn==0.0 in /opt/conda/lib/python3.9/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sklearn==0.0) (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn==0.0) (1.7.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn==0.0) (3.0.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn==0.0) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn==0.0) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.5\n",
    "!pip install sklearn==0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b80301",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485c0034",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, pow, abs, to_date, to_timestamp, min, max\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e256c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca16acf",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1a6ead88-559a-47fe-b043-9462fd94eb05;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 183ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1a6ead88-559a-47fe-b043-9462fd94eb05\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/7ms)\n",
      "21/12/19 13:19:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aff6e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Import data\n",
    "\n",
    "Load cryptocurrency price data from MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636d21a4",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+-----------------+\n",
      "|                 _id|             close|      date|             open|\n",
      "+--------------------+------------------+----------+-----------------+\n",
      "|{619f20a05fec62ac...|        61238.6222|2021-10-23|60690.28731420007|\n",
      "|{619f20a15fec62ac...| 60694.00000650834|2021-10-22|62196.96438572152|\n",
      "|{619f20a15fec62ac...|62298.093007718286|2021-10-21|65961.04976056097|\n",
      "+--------------------+------------------+----------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crypto_df = spark.read.format(\"mongo\").option(\"uri\",\"mongodb://mongo/seng550.crypto\").load()\n",
    "crypto_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7405198",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load news headlines from MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf62dd9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|                 _id|      date|            headline|\n",
      "+--------------------+----------+--------------------+\n",
      "|{619f1214b82509cf...|2021-08-01|Family speaks out...|\n",
      "|{619f1215b82509cf...|2021-08-01|China invasion of...|\n",
      "|{619f1215b82509cf...|2021-08-01|US digital paymen...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headline_df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://mongo/seng550.headers\").load()\n",
    "headline_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92500b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Clean data\n",
    "\n",
    "Compute the price change each day as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b1e3a6",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+-----------------+--------------------+\n",
      "|                 _id|             close|      date|             open|              change|\n",
      "+--------------------+------------------+----------+-----------------+--------------------+\n",
      "|{619f20a05fec62ac...|        61238.6222|2021-10-23|60690.28731420007|0.009034969351209954|\n",
      "|{619f20a15fec62ac...| 60694.00000650834|2021-10-22|62196.96438572152|-0.02416459378776714|\n",
      "|{619f20a15fec62ac...|62298.093007718286|2021-10-21|65961.04976056097|-0.05553211730467056|\n",
      "+--------------------+------------------+----------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crypto_df = crypto_df.withColumn('change', (col('close') / col('open')) - 1)\n",
    "crypto_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert string dates to date type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+-----------------+--------------------+\n",
      "|                 _id|             close|      date|             open|              change|\n",
      "+--------------------+------------------+----------+-----------------+--------------------+\n",
      "|{619f20a05fec62ac...|        61238.6222|2021-10-23|60690.28731420007|0.009034969351209954|\n",
      "|{619f20a15fec62ac...| 60694.00000650834|2021-10-22|62196.96438572152|-0.02416459378776714|\n",
      "|{619f20a15fec62ac...|62298.093007718286|2021-10-21|65961.04976056097|-0.05553211730467056|\n",
      "+--------------------+------------------+----------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crypto_df = crypto_df.withColumn('date', to_date(col('date')))\n",
    "crypto_df.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|                 _id|      date|            headline|\n",
      "+--------------------+----------+--------------------+\n",
      "|{619f1214b82509cf...|2021-08-01|Family speaks out...|\n",
      "|{619f1215b82509cf...|2021-08-01|China invasion of...|\n",
      "|{619f1215b82509cf...|2021-08-01|US digital paymen...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headline_df = headline_df.withColumn('date', to_date(col('date')))\n",
    "headline_df.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "f77077e4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pre-process news headlines. Punctuation is removed. Stop words are removed. Words are stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|                 _id|      date|            headline|      headline_clean|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|{619f1214b82509cf...|2021-08-01|Family speaks out...|famili speak raci...|\n",
      "|{619f1215b82509cf...|2021-08-01|China invasion of...|china invas taiwa...|\n",
      "|{619f1215b82509cf...|2021-08-01|US digital paymen...|us digit payment ...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def clean_headline(headline):\n",
    "    # Headers are small, so local process is ok.\n",
    "    porter = PorterStemmer()\n",
    "    headline = headline.strip()\n",
    "    if not headline:\n",
    "        return ''\n",
    "    headline = headline.translate(str.maketrans('', '', string.punctuation))\n",
    "    return ' '.join(porter.stem(word) for word in headline.split() if word not in ENGLISH_STOP_WORDS)\n",
    "\n",
    "udf_clean_header = udf(lambda s: clean_headline(s), StringType())\n",
    "headline_df = headline_df.withColumn('headline_clean', udf_clean_header(col('headline')))\n",
    "headline_df.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "id": "35486d87"
  },
  {
   "cell_type": "markdown",
   "id": "a607ff2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Distribute price changes evenly across news headlines each day.\n",
    "\n",
    "The training dataset does not attribute price changes to individual headlines. We approximate headline effects on price by attributing price changes to all headlines evenly. Distribution is done such that the product of all price changes attributed to individual headlines results in the original daily price change. Inaccuracies introduced by this approximation are mitigated by larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165c387e",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|      headline_clean|   change_normalized|\n",
      "+----------+--------------------+--------------------+\n",
      "|2021-08-27|murray remind man...|0.004533092696482388|\n",
      "|2021-08-27|ndp’ singh talk p...|0.004533092696482388|\n",
      "|2021-08-27|rfk assassin sirh...|0.004533092696482388|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_count_df = headline_df.groupby('date').count()\n",
    "crypto_headline_df = crypto_df.join(headline_df, on='date')\\\n",
    "    .join(date_count_df.withColumnRenamed('count', 'date_count'), on='date')\\\n",
    "    .withColumn('change_normalized', pow(1 + col('change'), 1 / col('date_count')) - 1)\\\n",
    "    .select('date', 'headline_clean', 'change_normalized')\n",
    "crypto_headline_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282af9c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Train model\n",
    "\n",
    "Split the dataset into 80% train and 20% test partitions. The model is trained on the training partition and evaluated on the test partition. The partition is done by time because it is timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "\n",
    "udf_unix_timestamp = udf(lambda date: int(date.strftime('%s')), LongType())\n",
    "crypto_headline_df = crypto_headline_df.withColumn('unix_timestamp', udf_unix_timestamp(col('date')))\n",
    "cutoff_timestamp = crypto_headline_df.approxQuantile('unix_timestamp', [train_fraction], 0)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51fe1ed9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|      headline_clean|   change_normalized|\n",
      "+----------+--------------------+--------------------+\n",
      "|2021-08-27|murray remind man...|0.004533092696482388|\n",
      "|2021-08-27|ndp’ singh talk p...|0.004533092696482388|\n",
      "|2021-08-27|rfk assassin sirh...|0.004533092696482388|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+----------+\n",
      "| min(date)| max(date)|\n",
      "+----------+----------+\n",
      "|2021-03-01|2021-09-06|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = crypto_headline_df.where(col('unix_timestamp') <= cutoff_timestamp).drop('unix_timestamp')\n",
    "train_df.show(3)\n",
    "train_df.agg(min(col('date')), max(col('date'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e46e78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|      headline_clean|   change_normalized|\n",
      "+----------+--------------------+--------------------+\n",
      "|2021-10-11|jesi nelson claim...|0.004835823570711...|\n",
      "|2021-10-11|elect worker geor...|0.004835823570711...|\n",
      "|2021-10-11|texa gov abbott i...|0.004835823570711...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+----------+\n",
      "| min(date)| max(date)|\n",
      "+----------+----------+\n",
      "|2021-09-07|2021-10-23|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = crypto_headline_df.where(col('unix_timestamp') > cutoff_timestamp).drop('unix_timestamp')\n",
    "test_df.show(3)\n",
    "test_df.agg(min(col('date')), max(col('date'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b27d51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fit a TF-IDF vectorizer to the train data. The TF-IDF vectorizer knows how to convert pre-processed text into a sparse vector of TF-IDF values based on a learned vocabulary. All terms and inverse document frequencies (IDFs) are established by the training data. The model does not learn new terms that are introduced after this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22aa943a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "class TfidfModel:\n    def __init__(self, tokenizer, cv_model, idf_model):\n        self.tokenizer = tokenizer\n        self.cv_model = cv_model\n        self.idf_model = idf_model\n\n# Use this prefix on temporary columns so they don't conflict with existing columns.\ntfidf_comp_prefix = 'kfwsxy'\n\ndef fit_tfidf_model(df, input_col, output_col):\n    tokenizer = Tokenizer(inputCol=input_col, outputCol=f'{tfidf_comp_prefix}_words')\n    words_data = tokenizer.transform(df)\n\n    cv = CountVectorizer(inputCol=f'{tfidf_comp_prefix}_words', outputCol=f'{tfidf_comp_prefix}_raw')\n    cv_model = cv.fit(words_data)\n\n    featurized_data = cv_model.transform(words_data)\n\n    idf = IDF(inputCol=f'{tfidf_comp_prefix}_raw', outputCol=output_col)\n    idf_model = idf.fit(featurized_data)\n\n    return TfidfModel(tokenizer, cv_model, idf_model)\n\ndef tfidf_vectorize(df, tfidf_model):\n    words_data = tfidf_model.tokenizer.transform(df)\n    featurized_data = tfidf_model.cv_model.transform(words_data)\n    res_df = tfidf_model.idf_model.transform(featurized_data)\n    return res_df\\\n        .drop(f'{tfidf_comp_prefix}_words')\\\n        .drop(f'{tfidf_comp_prefix}_raw').drop('headline_clean')\n\ntfidf_model = fit_tfidf_model(train_df, 'headline_clean', 'headline_features')"
  },
  {
   "cell_type": "markdown",
   "id": "01199b3f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "TF-IDF vectorize the training data. All headline terms will be encoded because the vectorizer vocabulary contains all training terms."
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "454786b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|   change_normalized|   headline_features|\n",
      "+----------+--------------------+--------------------+\n",
      "|2021-08-27|0.004533092696482388|(5604,[28,267,540...|\n",
      "|2021-08-27|0.004533092696482388|(5604,[75,143,146...|\n",
      "|2021-08-27|0.004533092696482388|(5604,[38,56,80,1...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": "vectorized_train_df = tfidf_vectorize(train_df, tfidf_model)\nvectorized_train_df.show(3)"
  },
  {
   "cell_type": "markdown",
   "id": "54eed701",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "TF-IDF vectorize the test data. Headline terms not seen before will be dropped."
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "575b35b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|   change_normalized|   headline_features|\n",
      "+----------+--------------------+--------------------+\n",
      "|2021-10-11|0.004835823570711...|(5604,[54,292,346...|\n",
      "|2021-10-11|0.004835823570711...|(5604,[225,233,26...|\n",
      "|2021-10-11|0.004835823570711...|(5604,[0,4,15,143...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": "vectorized_test_df = tfidf_vectorize(test_df, tfidf_model)\nvectorized_test_df.show(3)"
  },
  {
   "cell_type": "markdown",
   "id": "aaf78cba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Denote columns that contain raw features (TF-IDF vector), attributed affect on price, and predicted affect on price, respectively."
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "344bd402",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "features_col= 'headline_features'\nlabel_col= 'change_normalized'\nprediction_col = 'change_prediction'"
  },
  {
   "cell_type": "markdown",
   "id": "3f6c5aa0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Fit a ridge regression model to the training data. The ridge λ hyperparameter is tuned by trying many possibilities and selecting the best. Cross validation is used to ensure an accurate performance measure is determined for each λ tested. The training data is further segmented into train and validation partitions on each cross validation k-fold.\n\nThis cell runs very slowly. If you do not care about accuracy, set:\n- `num_folds = 2`\n- `num_lambda = 2`"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3133db57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/19 13:20:22 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "21/12/19 13:20:22 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "21/12/19 13:20:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/12/19 13:20:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/opt/conda/lib/python3.9/multiprocessing/pool.py\u001B[0m in \u001B[0;36mnext\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    852\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 853\u001B[0;31m                 \u001B[0mitem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_items\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpopleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    854\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mIndexError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3077/1177914512.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m     parallelism=8)\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0mlr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcross_val\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvectorized_train_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    687\u001B[0m                 \u001B[0minheritable_thread_target\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    688\u001B[0m                 _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001B[0;32m--> 689\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubModel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimap_unordered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    690\u001B[0m                 \u001B[0mmetrics\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmetric\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mnFolds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    691\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/multiprocessing/pool.py\u001B[0m in \u001B[0;36mnext\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    856\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pool\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    857\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 858\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cond\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    859\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    860\u001B[0m                     \u001B[0mitem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_items\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpopleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    311\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 312\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    313\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    314\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": "num_folds = 3\nnum_lambda = 50\n\nreg_params = np.linspace(0.07, 0.10, num_lambda) \n\nlr_base = LinearRegression(\n    elasticNetParam=0,\n    featuresCol=features_col,\n    labelCol=label_col,\n    predictionCol=prediction_col)\n\npipeline = Pipeline(stages=[lr_base])\n\nparam_grid = ParamGridBuilder()\\\n    .addGrid(lr_base.regParam, reg_params)\\\n    .build()\n\npipeline_evaluator = RegressionEvaluator(\n    labelCol=label_col,\n    predictionCol=prediction_col)\n\ncross_val = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=param_grid,\n    evaluator=pipeline_evaluator,\n    numFolds=num_folds,\n    parallelism=8)\n\nlr = cross_val.fit(vectorized_train_df)"
  },
  {
   "cell_type": "markdown",
   "id": "c62c1def",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Graph the performance of all ridge λ hyperparameters tested. The value yielding the lowest error is the best."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b289bc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "x_label = 'λ'\ny_label = 'Average ' + pipeline_evaluator.getMetricName()\ntune_result_df = pd.DataFrame({\n    x_label: reg_params,\n    y_label: lr.avgMetrics})\\\n    .set_index(['λ'])\nplot = tune_result_df.plot.line(\n    title='Cross validation error by ridge regression λ parameter',\n    ylabel=y_label,\n    legend=False)"
  },
  {
   "cell_type": "markdown",
   "id": "7f803dc8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 5. Results\n\n### Daily price change statistics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6d373",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "crypto_df.describe('change').show(truncate=False)"
  },
  {
   "cell_type": "markdown",
   "source": "### Daily price change percentiles",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "05eaef04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "percentiles = ['1.00', '0.99', '0.95', '0.90', '0.10', '0.05', '0.01', '0.00']\npercentile_changes = crypto_df.approxQuantile('change', list(map(lambda p: float(p), percentiles)), 0)\nspark.createDataFrame(zip(percentiles, percentile_changes), ['percentile', 'change']).show(truncate=False)",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "id": "688574f1"
  },
  {
   "cell_type": "markdown",
   "id": "58b31862",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Sample of predictions made with training data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10d205",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "train_prediction_df = lr.transform(vectorized_train_df)\ntrain_prediction_df.show(3)"
  },
  {
   "cell_type": "markdown",
   "id": "ba0a3730",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Sample of predictions made with test data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3f53e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "test_prediction_df = lr.transform(vectorized_test_df)\ntest_prediction_df.show(3)"
  },
  {
   "cell_type": "markdown",
   "id": "f5081f33",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Training errors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bae462",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def lr_evaluator(metric_name):\n    return RegressionEvaluator(\n        labelCol=label_col,\n        predictionCol=prediction_col,\n        metricName=metric_name)\n\ndef evaluate_lr(prediction_df, metrics=None):\n    if metrics is None:\n        metrics = ['rmse', 'mse', 'mae', 'var']\n    scores = {}\n    for metric in metrics:\n        evaluator = lr_evaluator(metric)\n        scores[metric] = evaluator.evaluate(prediction_df)\n    df = spark.createDataFrame(scores.items(), ['metric', 'score'])\n    return df\n\nevaluate_lr(train_prediction_df).show(truncate=False)"
  },
  {
   "cell_type": "markdown",
   "id": "a4cfaa22",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Test errors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509c5d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "evaluate_lr(test_prediction_df).show(truncate=False)"
  },
  {
   "cell_type": "markdown",
   "id": "a4ad0fc3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Top 10 terms by coefficient magnitude"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5c445",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "lr_inner = lr.bestModel.stages[0]\ncoefficients_zip = zip(\n    tfidf_model.cv_model.vocabulary,\n    map(lambda x: float(x), lr_inner.coefficients))\ncoefficients_df = spark.createDataFrame(coefficients_zip, ['term', 'coefficient'])\n\ncoefficients_df\\\n    .withColumn('coefficient_magnitude', abs('coefficient'))\\\n    .orderBy('coefficient_magnitude', ascending=False)\\\n    .drop('coefficient_magnitude')\\\n    .show(10, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "id": "ed8a934d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Term coefficient statistics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444589d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "coefficients_df.describe('coefficient').show(truncate=False)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}